# TensorFlow Lite Micro Speech æ¨¡å‹è¨“ç·´ - Google Colab ç‰ˆæœ¬
# ç›´æ¥è¤‡è£½åˆ° Colab åŸ·è¡Œï¼šhttps://colab.research.google.com/

# ============================================================================
# æ­¥é©Ÿ 1: å•Ÿç”¨ GPUï¼ˆé‡è¦ï¼è¨“ç·´é€Ÿåº¦å¿« 10 å€ï¼‰
# ============================================================================
# Runtime â†’ Change runtime type â†’ Hardware accelerator: GPU â†’ Save

# ============================================================================
# æ­¥é©Ÿ 2: é…ç½®åƒæ•¸
# ============================================================================

# è¨“ç·´é—œéµå­—ï¼ˆå¯ä¿®æ”¹ï¼‰
WANTED_WORDS = "yes,no"  # é¸é …: yes,no,up,down,left,right,on,off,stop,go

# è¨“ç·´æ­¥æ•¸
TRAINING_STEPS = "12000,3000"  # ç¸½å…± 15000 æ­¥ï¼Œç´„ 15-20 åˆ†é˜ï¼ˆGPUï¼‰
LEARNING_RATE = "0.001,0.0001"

print(f"è¨“ç·´é—œéµå­—: {WANTED_WORDS}")
print(f"è¨“ç·´æ­¥æ•¸: {TRAINING_STEPS}")

# ============================================================================
# æ­¥é©Ÿ 3: ä¸‹è¼‰ TensorFlow å€‰åº«
# ============================================================================

!git clone -q --depth 1 https://github.com/tensorflow/tensorflow
print("âœ… TensorFlow å€‰åº«ä¸‹è¼‰å®Œæˆ")

# ============================================================================
# æ­¥é©Ÿ 4: é–‹å§‹è¨“ç·´
# ============================================================================

!python tensorflow/tensorflow/examples/speech_commands/train.py \
--data_dir=dataset/ \
--wanted_words={WANTED_WORDS} \
--silence_percentage=25 \
--unknown_percentage=25 \
--preprocess=micro \
--window_stride=20 \
--model_architecture=tiny_conv \
--how_many_training_steps={TRAINING_STEPS} \
--learning_rate={LEARNING_RATE} \
--train_dir=train/ \
--summaries_dir=logs/ \
--verbosity=INFO \
--eval_step_interval=1000 \
--save_step_interval=1000

print("\nâœ… è¨“ç·´å®Œæˆï¼")

# ============================================================================
# æ­¥é©Ÿ 5: å‡çµæ¨¡å‹
# ============================================================================

TOTAL_STEPS = str(sum(map(lambda x: int(x), TRAINING_STEPS.split(","))))

!python tensorflow/tensorflow/examples/speech_commands/freeze.py \
--wanted_words={WANTED_WORDS} \
--window_stride_ms=20 \
--preprocess=micro \
--model_architecture=tiny_conv \
--start_checkpoint=train/tiny_conv.ckpt-{TOTAL_STEPS} \
--save_format=saved_model \
--output_file=models/saved_model

print("âœ… æ¨¡å‹å‡çµå®Œæˆ")

# ============================================================================
# æ­¥é©Ÿ 6: è½‰æ›ç‚º TFLite ä¸¦æ¸¬è©¦
# ============================================================================

import sys
import tensorflow as tf
import numpy as np

# æ·»åŠ è·¯å¾‘
sys.path.append("/content/tensorflow/tensorflow/examples/speech_commands/")
import input_data
import models as model_utils

# æº–å‚™æ¨¡å‹è¨­ç½®
SAMPLE_RATE = 16000
CLIP_DURATION_MS = 1000
WINDOW_SIZE_MS = 30.0
WINDOW_STRIDE = 20
FEATURE_BIN_COUNT = 40
PREPROCESS = 'micro'

DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'
VALIDATION_PERCENTAGE = 10
TESTING_PERCENTAGE = 10
SILENT_PERCENTAGE = 25
UNKNOWN_PERCENTAGE = 25

model_settings = model_utils.prepare_model_settings(
    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),
    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,
    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS
)

audio_processor = input_data.AudioProcessor(
    DATA_URL, 'dataset/',
    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,
    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,
    TESTING_PERCENTAGE, model_settings, 'logs/'
)

# å‰µå»ºç›®éŒ„
!mkdir -p models

# ç”Ÿæˆæµ®é»æ¨¡å‹
print("\n[è½‰æ›] æµ®é»æ¨¡å‹...")
with tf.compat.v1.Session() as sess:
    float_converter = tf.lite.TFLiteConverter.from_saved_model('models/saved_model')
    float_tflite_model = float_converter.convert()
    with open('models/float_model.tflite', 'wb') as f:
        f.write(float_tflite_model)
    print(f"âœ… æµ®é»æ¨¡å‹: {len(float_tflite_model)} bytes")

# ç”Ÿæˆé‡åŒ–æ¨¡å‹ï¼ˆint8ï¼‰
print("\n[è½‰æ›] é‡åŒ–æ¨¡å‹...")
with tf.compat.v1.Session() as sess:
    converter = tf.lite.TFLiteConverter.from_saved_model('models/saved_model')
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.inference_input_type = tf.int8
    converter.inference_output_type = tf.int8
    
    def representative_dataset_gen():
        for i in range(100):
            data, _ = audio_processor.get_data(
                1, i*1, model_settings,
                0.8, 0.1, 100.0, 'testing', sess
            )
            flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)
            yield [flattened_data]
    
    converter.representative_dataset = representative_dataset_gen
    tflite_model = converter.convert()
    
    with open('models/model.tflite', 'wb') as f:
        f.write(tflite_model)
    print(f"âœ… é‡åŒ–æ¨¡å‹: {len(tflite_model)} bytes")

# ============================================================================
# æ­¥é©Ÿ 7: æ¸¬è©¦æ¨¡å‹æº–ç¢ºåº¦
# ============================================================================

def test_model(tflite_path, model_type="Float"):
    print(f"\n[æ¸¬è©¦] {model_type} æ¨¡å‹æº–ç¢ºåº¦...")
    
    np.random.seed(0)
    with tf.compat.v1.Session() as sess:
        test_data, test_labels = audio_processor.get_data(
            -1, 0, model_settings, 0.8, 0.1, 100.0, 'testing', sess
        )
    test_data = np.expand_dims(test_data, axis=1).astype(np.float32)
    
    interpreter = tf.lite.Interpreter(
        tflite_path,
        experimental_op_resolver_type=tf.lite.experimental.OpResolverType.BUILTIN_REF
    )
    interpreter.allocate_tensors()
    
    input_details = interpreter.get_input_details()[0]
    output_details = interpreter.get_output_details()[0]
    
    if model_type == "Quantized":
        input_scale, input_zero_point = input_details["quantization"]
        test_data = test_data / input_scale + input_zero_point
        test_data = test_data.astype(input_details["dtype"])
    
    correct = 0
    for i in range(len(test_data)):
        interpreter.set_tensor(input_details["index"], test_data[i])
        interpreter.invoke()
        output = interpreter.get_tensor(output_details["index"])[0]
        if output.argmax() == test_labels[i]:
            correct += 1
    
    accuracy = (correct * 100) / len(test_data)
    print(f"âœ… {model_type} æº–ç¢ºåº¦: {accuracy:.2f}% (æ¸¬è©¦æ¨£æœ¬: {len(test_data)})")

test_model('models/float_model.tflite', 'Float')
test_model('models/model.tflite', 'Quantized')

# ============================================================================
# æ­¥é©Ÿ 8: ç”Ÿæˆ C é™£åˆ—
# ============================================================================

print("\n[è½‰æ›] ç”Ÿæˆ C é™£åˆ—...")

with open('models/model.tflite', 'rb') as f:
    tflite_binary = f.read()

hex_array = ', '.join([f'0x{b:02x}' for b in tflite_binary])

c_code = f"""// è‡ªå‹•ç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶
// è¨“ç·´é—œéµå­—: {WANTED_WORDS}
// æ¨¡å‹å¤§å°: {len(tflite_binary)} bytes

#include <stdint.h>

// æ¨¡å‹æ•¸æ“š
alignas(8) const unsigned char g_model[] = {{
  {hex_array}
}};

const int g_model_len = {len(tflite_binary)};
"""

with open('models/model.cc', 'w') as f:
    f.write(c_code)

print(f"âœ… C é™£åˆ—å·²ç”Ÿæˆ: models/model.cc ({len(tflite_binary)} bytes)")

# ============================================================================
# æ­¥é©Ÿ 9: ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ
# ============================================================================

print("\n" + "="*70)
print("  è¨“ç·´å®Œæˆï¼ğŸ‰")
print("="*70)
print("\nç”Ÿæˆçš„æª”æ¡ˆ:")
print("  - models/float_model.tflite (æµ®é»æ¨¡å‹)")
print("  - models/model.tflite (é‡åŒ–æ¨¡å‹)")
print("  - models/model.cc (C é™£åˆ—ï¼Œç”¨æ–¼ ESP32)")
print("\nä¸‹è¼‰æ–¹å¼:")
print("  å·¦å´é¸å–® Files â†’ models â†’ å³éµä¸‹è¼‰")
print("\néƒ¨ç½²åˆ° ESP32:")
print("  1. ä¸‹è¼‰ model.cc")
print("  2. è¤‡è£½åˆ°æ‚¨çš„ ESP32 å°ˆæ¡ˆ src/ ç›®éŒ„")
print("  3. æ›¿æ›åŸæœ‰çš„æ¨¡å‹æª”æ¡ˆ")
print("  4. æ›´æ–°é¡åˆ¥æ•¸é‡å’Œæ¨™ç±¤")
print("  5. é‡æ–°ç·¨è­¯ä¸¦ä¸Šå‚³")
print("="*70)

# é¡¯ç¤ºæª”æ¡ˆåˆ—è¡¨
!ls -lh models/
