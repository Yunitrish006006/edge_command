# üéØ Speech Recognition Model Accuracy Improvement Guide

## Current Performance
- Training Accuracy: **85%**
- Validation Accuracy: **83%**
- Testing Accuracy: **82%**
- Overfitting: **2%** (Healthy)
- **Goal: Improve to 90%+**

---

## üöÄ Improvement Methods (Ranked by Effectiveness)

### ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Method 1: Increase Training Steps
**Most Effective & Easiest**

**Current Configuration:**
```python
TRAINING_STEPS = "15000,3000"  # Total: 18,000 steps
```

**Recommended Configuration:**
```python
TRAINING_STEPS = "30000,6000"  # Total: 36,000 steps (2x increase)
```

**Expected Improvement:** +3-5% accuracy
**Training Time:** 4-6 hours (2x longer)

**Why it works:**
- More iterations allow the model to learn better patterns
- 13 keywords need more training than 2 keywords
- Current 18k steps is insufficient for 13 classes

---

### ‚≠ê‚≠ê‚≠ê‚≠ê Method 2: Use Larger Model Architecture
**Better Capacity for Complex Patterns**

**Current Configuration:**
```python
MODEL_ARCHITECTURE = 'tiny_conv'  # ~18 KB
```

**Recommended Options:**

#### Option A: Low Latency Conv (Recommended)
```python
MODEL_ARCHITECTURE = 'low_latency_conv'  # ~25 KB, ~90% accuracy
```
- **Size:** 25 KB (fits ESP32)
- **Expected Accuracy:** 88-92%
- **Inference Time:** 100-120 ms on ESP32

#### Option B: Conv Model (Best Accuracy)
```python
MODEL_ARCHITECTURE = 'conv'  # ~80 KB, ~93% accuracy
```
- **Size:** 80 KB (might be too large for ESP32)
- **Expected Accuracy:** 91-95%
- **Inference Time:** 150-200 ms

**Available Models (from small to large):**
1. `tiny_conv` - 18 KB, ~85% (current)
2. `low_latency_conv` - 25 KB, ~90% ‚≠ê **RECOMMENDED**
3. `low_latency_svdf` - 30 KB, ~91%
4. `conv` - 80 KB, ~93%
5. `ds_cnn` - 100+ KB, ~95% (too large)

---

### ‚≠ê‚≠ê‚≠ê Method 3: Advanced Learning Rate Schedule
**Fine-tune Training Process**

**Current Configuration:**
```python
TRAINING_STEPS = "15000,3000"
LEARNING_RATE = "0.001,0.0001"
```

**Recommended Configuration:**
```python
TRAINING_STEPS = "20000,10000,6000"  # 3-stage training
LEARNING_RATE = "0.0005,0.0001,0.00002"  # Gradual decay
```

**Expected Improvement:** +1-2% accuracy

**Why it works:**
- Start with moderate learning rate (0.0005) for stable learning
- Mid-stage fine-tuning (0.0001)
- Final refinement (0.00002) for best performance

---

### ‚≠ê‚≠ê‚≠ê Method 4: Data Augmentation
**Increase Training Data Variety**

Add these parameters to training script:

```python
# Add to training command
BACKGROUND_FREQUENCY = 0.9      # Increase from 0.8
BACKGROUND_VOLUME_RANGE = 0.2   # Increase from 0.1
TIME_SHIFT_MS = 150.0           # Increase from 100.0
```

**Expected Improvement:** +2-3% accuracy

**Why it works:**
- More realistic background noise
- Better volume variation
- Time shifting improves robustness

---

### ‚≠ê‚≠ê Method 5: Reduce Number of Keywords
**Simplify the Task**

**Current:** 13 keywords
**Recommendation:** Group similar keywords

**Option A: Reduce to 10 keywords**
```python
WANTED_WORDS = "on,off,zero,one,two,three,four,five,nine,house"
# Remove: six, seven, eight (harder to distinguish)
```

**Option B: Reduce to 7 keywords**
```python
WANTED_WORDS = "on,off,one,two,three,four,five"
# Keep essential commands only
```

**Expected Improvement:** +5-8% accuracy
**Trade-off:** Fewer commands available

---

## üéØ Recommended Configuration for 90%+ Accuracy

### Configuration A: Balanced (Recommended)
```python
WANTED_WORDS = "on,off,zero,one,two,three,four,five,six,seven,eight,nine,house"
TRAINING_STEPS = "30000,6000"
LEARNING_RATE = "0.001,0.0001"
MODEL_ARCHITECTURE = 'low_latency_conv'
```

**Expected Results:**
- Accuracy: 88-92%
- Model Size: ~25 KB
- Training Time: 4-6 hours
- Inference: 100-120 ms

### Configuration B: Maximum Accuracy
```python
WANTED_WORDS = "on,off,zero,one,two,three,four,five,six,seven,eight,nine,house"
TRAINING_STEPS = "40000,8000"
LEARNING_RATE = "0.0005,0.0001"
MODEL_ARCHITECTURE = 'low_latency_svdf'
BACKGROUND_FREQUENCY = 0.9
BACKGROUND_VOLUME_RANGE = 0.2
TIME_SHIFT_MS = 150.0
```

**Expected Results:**
- Accuracy: 90-94%
- Model Size: ~30 KB
- Training Time: 6-8 hours
- Inference: 120-150 ms

### Configuration C: Fast & Good Enough
```python
WANTED_WORDS = "on,off,one,two,three,four,five"  # 7 keywords
TRAINING_STEPS = "20000,4000"
LEARNING_RATE = "0.001,0.0001"
MODEL_ARCHITECTURE = 'low_latency_conv'
```

**Expected Results:**
- Accuracy: 92-96%
- Model Size: ~25 KB
- Training Time: 2-3 hours
- Inference: 80-100 ms

---

## üìä Comparison Table

| Method | Accuracy Gain | Training Time | Model Size | Difficulty |
|--------|--------------|---------------|------------|------------|
| **Increase Steps (2x)** | +3-5% | 2x | Same | ‚≠ê Easy |
| **Low Latency Conv** | +5-7% | Same | +7 KB | ‚≠ê Easy |
| **3-Stage Learning** | +1-2% | Same | Same | ‚≠ê‚≠ê Medium |
| **Data Augmentation** | +2-3% | Same | Same | ‚≠ê‚≠ê‚≠ê Hard |
| **Reduce Keywords** | +5-8% | 0.5x | Same | ‚≠ê Easy |

---

## üîß How to Apply Changes

### Quick Start (Recommended)
Already applied in `train_with_visualization.py`:
```python
MODEL_ARCHITECTURE = 'low_latency_conv'
TRAINING_STEPS = "30000,6000"
```

### Run Training
```bash
python .\python\train_with_visualization.py
```

### Monitor Progress
Training will take 4-6 hours. Check progress with:
```bash
# View last 20 lines of training output
Get-Content train_output.log -Tail 20
```

---

## üí° Additional Tips

1. **Check Dataset Quality**
   ```bash
   # Verify all keyword folders have enough samples
   Get-ChildItem dataset\ | Where-Object {$_.Name -in @('on','off','zero','one','two','three','four','five','six','seven','eight','nine','house')} | Select-Object Name, @{N='Count';E={(Get-ChildItem $_.FullName -Filter *.wav).Count}}
   ```

2. **Monitor Training**
   - Watch for accuracy plateau
   - Stop early if validation accuracy stops improving
   - Use TensorBoard for detailed monitoring

3. **Test on Real Device**
   - PC accuracy != ESP32 accuracy
   - Test with actual microphone
   - Consider environmental noise

---

## üìà Expected Timeline

| Configuration | Training Time | Expected Accuracy | Best Use Case |
|--------------|---------------|-------------------|---------------|
| **Current** | 2-3 hours | 82-85% | Quick test |
| **Recommended** | 4-6 hours | 88-92% | Production |
| **Maximum** | 6-8 hours | 90-94% | High accuracy needed |
| **Simplified** | 2-3 hours | 92-96% | Fewer commands ok |

---

## üéØ Action Plan

### Immediate (Already Done ‚úÖ)
- [x] Changed to `low_latency_conv` model
- [x] Increased training steps to 36,000

### Next Steps
1. **Run the training** (4-6 hours)
2. **Check results** - Target: 88-92%
3. **If still < 90%:**
   - Add 3-stage learning rate
   - Increase data augmentation
   - Consider reducing keywords

### If Results Are Good (‚â•90%)
- Deploy to ESP32
- Test with real microphone
- Fine-tune based on real-world performance

### If Results Are Still Low (<88%)
- Try `low_latency_svdf` model
- Increase to 48,000 training steps
- Verify dataset quality
- Consider reducing to 7-10 keywords

---

## üìû Troubleshooting

### Problem: Training Time Too Long
**Solution:** Reduce keywords or use GPU

### Problem: Model Too Large for ESP32
**Solution:** Use `tiny_conv` or quantize more aggressively

### Problem: Accuracy Plateaus Early
**Solution:** Lower initial learning rate to 0.0005

### Problem: High Overfitting (>5%)
**Solution:** Add more data augmentation

---

Generated: 2025-10-18
Based on training results for 13 keywords (on,off,0-10,house)
